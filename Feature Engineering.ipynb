{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPuLkuHpZWz2C/4IBfLfOAS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Feature Engineering**\n","# Assignment Questions"],"metadata":{"id":"XYkbrV62Rcry"}},{"cell_type":"markdown","source":["**Q1:What is a parameter?**\n","\n","---> A **parameter** is a variable that the model learns from the training data to make predictions. For instance, in a simple linear regression model represented by the equation \\( y = mx + b \\), the parameters are \\( m \\) (slope) and \\( b \\) (intercept). These parameters are adjusted during training to best fit the data.\n","\n","It's important to distinguish parameters from **hyperparameters**, which are set before training and control aspects of the learning process, such as the learning rate or the number of layers in a neural network."],"metadata":{"id":"8LuByEBfRmXw"}},{"cell_type":"markdown","source":["**Q2:What is correlation?**\n","\n","Correlation is a statistical measure that describes how two variables move in relation to each other. It indicates whether an increase or decrease in one variable corresponds to an increase or decrease in another variable.\n","\n","**Types of Correlation:**\n","\n","1. **Positive Correlation:** Both variables increase or decrease together. For example, as the number of study hours increases, test scores might also increase.\n","\n","2. **Negative Correlation:** As one variable increases, the other decreases. For instance, as the speed of a car increases, the time taken to reach the destination decreases.\n","\n","3. **Zero Correlation:** No discernible relationship exists between the variables; changes in one do not predict changes in the other.\n","\n","The strength and direction of a linear relationship between two variables are quantified by the **correlation coefficient**, denoted as \\( r \\). This coefficient ranges from -1 to +1:\n","\n","- \\( r = +1 \\): Perfect positive correlation.\n","\n","- \\( r = -1 \\): Perfect negative correlation.\n","\n","- \\( r = 0 \\): No linear correlation.\n","\n","**Q: What does negative correlation mean?**\n","\n","A **negative correlation** describes a relationship between two variables where an increase in one variable is associated with a decrease in the other, and vice versa. This inverse relationship means that as one variable rises, the other tends to fall.\n","**Examples of Negative Correlation:**\n","\n","- **Temperature and Hot Beverage Sales:** As outdoor temperatures increase, sales of hot beverages like coffee or tea often decrease, since people prefer cooler drinks in warmer weather.\n","\n","- **Exercise and Body Weight:** An increase in physical exercise is often associated with a decrease in body weight, assuming other factors remain constant.\n","\n","The strength and direction of a correlation are measured by the **correlation coefficient**, denoted as \\( r \\), which ranges from -1 to +1:\n","\n","- **\\( r = -1 \\):** Indicates a perfect negative correlation, meaning the variables move in exactly opposite directions.\n","\n","- **\\( r = 0 \\):** Indicates no correlation; the variables do not have a predictable relationship.\n","\n","- **\\( r = +1 \\):** Indicates a perfect positive correlation, meaning the variables move in the same direction.\n"],"metadata":{"id":"h2T1_qBfSq8Y"}},{"cell_type":"markdown","source":["**Q3:Define Machine Learning. What are the main components in Machine Learning?**\n","\n","Machine learning is a branch of artificial intelligence that enables computers to learn from data and improve their performance over time without being explicitly programmed.It involves developing algorithms that can identify patterns within data and make predictions or decisions based on that information.\n","\n","**Main Components of Machine Learning:**\n","\n","1. **Data:** The foundational element of machine learning. High-quality, relevant data is essential for training models effectively.\n","\n","2. **Features:** Individual measurable properties or characteristics of the data used by the model to make predictions or classifications.\n","\n","3. **Model:** The mathematical or computational structure that learns from the data. It identifies patterns and makes predictions or decisions based on the input features.\n","\n","4. **Training:** The process of feeding data into the model and allowing it to learn by adjusting its parameters to minimize errors.\n","\n","5. **Evaluation:** Assessing the model's performance using metrics to determine its accuracy and generalization ability on new, unseen data.\n","\n","6. **Prediction/Inference:** Using the trained model to make predictions or decisions based on new input data."],"metadata":{"id":"UAZeTNTJTb1z"}},{"cell_type":"markdown","source":["**Q4:How does loss value help in determining whether the model is good or not?**\n","\n","In machine learning, the **loss value** is a numerical measure of how well a model's predictions align with the actual outcomes. It quantifies the errors made by the model: a lower loss value suggests that the model's predictions are closer to the true values, indicating better performance. Conversely, a higher loss value points to greater discrepancies between predictions and actual results, implying poorer performance.\n","\n","During training, the model's parameters are adjusted to minimize this loss, a process known as optimization. By continuously reducing the loss value, the model improves its ability to make accurate predictions on both the training data and, ideally, on unseen data.\n","\n","It's important to note that while a decreasing loss value during training typically indicates improving model performance, relying solely on the loss value isn't sufficient to determine if a model is \"good.\" For instance, a model might achieve a very low loss on training data but perform poorly on new, unseen data—a situation known as overfitting. Therefore, evaluating the loss on separate validation or test datasets is crucial to assess how well the model generalizes.\n","In summary, the loss value is a fundamental metric that guides the training process by indicating how well the model's predictions match actual outcomes. Monitoring and analyzing loss values help in understanding model performance and in making necessary adjustments to enhance accuracy and generalization."],"metadata":{"id":"eAG19dTJT2YH"}},{"cell_type":"markdown","source":["**Q5:What are continuous and categorical variables?**\n","\n","In statistics, variables are characteristics or attributes that can assume different values. They are primarily classified into two types: **categorical variables** and **continuous variables**.\n","\n","**Categorical Variables:**\n","\n","Categorical variables represent distinct groups or categories. These variables can be divided into:\n","\n","- **Nominal Variables:** Categories without any inherent order. Examples include eye color (blue, green, brown) or types of cuisine (Italian, Chinese, Mexican).\n","\n","- **Ordinal Variables:** Categories with a meaningful order but without consistent intervals between them. Examples include education levels (high school, bachelor's, master's) or customer satisfaction ratings (satisfied, neutral, dissatisfied).\n","\n","**Continuous Variables:**\n","\n","Continuous variables are numerical and can take any value within a range. They are typically measurements and can be divided into:\n","\n","- **Interval Variables:** Numerical scales with equal intervals between values but no true zero point. A common example is temperature measured in Celsius or Fahrenheit, where zero does not indicate the absence of temperature.\n","\n","- **Ratio Variables:** Similar to interval variables but with a meaningful zero point, indicating the absence of the measured attribute. Examples include height, weight, and age.\n"],"metadata":{"id":"k-sTxnVMUQGk"}},{"cell_type":"markdown","source":["**Q6:How do we handle categorical variables in Machine Learning? What are the common techniques**\n","\n","In machine learning, categorical variables represent data that can be divided into distinct groups or categories, such as colors, brands, or types. Since many machine learning algorithms require numerical input, it's essential to convert these categorical variables into numerical formats—a process known as **encoding**. Proper encoding ensures that models can interpret and utilize categorical data effectively\n","\n","**Common Techniques for Handling Categorical Variables:**\n","\n","1. **Label Encoding:**\n","   - Assigns a unique integer to each category. For example, 'Red', 'Green', and 'Blue' could be encoded as 0, 1, and 2, respectively.\n","   - Suitable for ordinal data where categories have a meaningful order.\n","   - However, for nominal data without an inherent order, label encoding might introduce unintended ordinal relationships.\n","\n","2. **One-Hot Encoding:**\n","   - Creates binary columns for each category, indicating the presence (1) or absence (0) of that category.\n","   - Effective for nominal data where categories do not have a specific order.\n","   - Can lead to a high-dimensional feature space when dealing with variables that have many unique categories (high cardinality).\n","\n","3. **Ordinal Encoding:**\n","   - Similar to label encoding but specifically used for ordinal data. Categories are mapped to integers based on their order. For instance, 'Low', 'Medium', and 'High' might be encoded as 0, 1, and 2.\n","   - Preserves the ordinal relationship among categories.\n","\n","4. **Target Encoding (Mean Encoding):**\n","   - Replaces each category with the mean of the target variable for that category.\n","   - Useful when there's a strong relationship between the categorical feature and the target variable.\n","   - Requires caution to avoid data leakage; it's advisable to perform this encoding using only the training data.\n","\n","5. **Frequency (Count) Encoding:**\n","   - Assigns the frequency or count of each category as its value.\n","   - Beneficial for handling high-cardinality features by reducing the dimensionality while retaining information about category prevalence.\n","\n","6. **Binary Encoding:**\n","   - Converts categories into binary digits and represents them in separate columns. For example, if a category is encoded as 5, its binary form (101) would be split into separate features.\n","   - Offers a compromise between one-hot encoding and label encoding, reducing dimensionality while preserving uniqueness.\n","\n","7. **Embedding Techniques:**\n","   - Utilize algorithms to learn dense vector representations (embeddings) of categories, capturing relationships between them.\n","   - Commonly used in deep learning models to handle categorical variables with high cardinality.\n","\n","The choice of encoding technique depends on factors such as the nature of the categorical variable (nominal or ordinal), the number of unique categories, and the specific requirements of the machine learning model being used. Proper handling of categorical variables is crucial for building effective and accurate predictive models."],"metadata":{"id":"Nlro9_UNVNeB"}},{"cell_type":"markdown","source":["**Q7:What do you mean by training and testing a dataset?**\n","\n","In machine learning, **training** and **testing** datasets are essential for developing and evaluating models.\n","\n","**Training Dataset:**\n","This subset of data is used to teach the model by adjusting its internal parameters to recognize patterns and relationships within the data. For example, in a supervised learning scenario, the training dataset includes input-output pairs that allow the model to learn the mapping from inputs to desired outputs. The goal during training is to enable the model to generalize from the training data to make accurate predictions on new, unseen data.\n","\n","**Testing Dataset:**\n","After training, the model's performance is evaluated using the testing dataset, which consists of new data that the model hasn't encountered during training. This evaluation assesses how well the model generalizes to unseen data, providing an unbiased measure of its predictive capabilities. A model that performs well on the testing dataset is considered to have good generalization ability.\n","\n","**Data Splitting:**\n","To effectively train and test a model, the original dataset is typically divided into these subsets:\n","\n","- **Training Set:** Usually comprises the majority of the data (e.g., 70-80%) and is used for learning.\n","\n","- **Testing Set:** The remaining portion (e.g., 20-30%) reserved for evaluating the model's performance.\n","\n","In some cases, a third subset called the **validation set** is used during training to fine-tune model parameters and prevent overfitting. This set helps in model selection and hyperparameter tuning without compromising the integrity of the testing set.\n","\n","Properly splitting data into training and testing sets is crucial to ensure that the model can generalize well to new data and perform effectively in real-world applications.\n"],"metadata":{"id":"lE_F87bHVzmh"}},{"cell_type":"markdown","source":["**Q8:What is sklearn.preprocessing?**\n","\n","In machine learning, preparing your data properly is crucial for building effective models. The sklearn.preprocessing module from the scikit-learn library offers a suite of tools to transform raw data into a format that's more suitable for modeling. These preprocessing techniques enhance the performance and accuracy of various machine learning algorithms.\n","\n","**Key Features of `sklearn.preprocessing`:**\n","\n","1. **Standardization and Scaling:**\n","   - **StandardScaler:** Adjusts features to have zero mean and unit variance, ensuring that each feature contributes equally to the model. This is particularly beneficial for algorithms sensitive to feature magnitudes, such as support vector machines and k-nearest neighbors.\n","   - **MinMaxScaler:** Scales features to a specified range, typically [0, 1], which is useful when the model requires normalized input features.\n","   - **RobustScaler:** Utilizes the median and interquartile range to scale features, making it effective for data with outliers.\n","\n","2. **Normalization:**\n","   - **Normalizer:** Rescales each data point to have a unit norm (e.g., length of 1), which is advantageous when working with datasets where the magnitude of feature vectors varies significantly.\n","\n","3. **Encoding Categorical Variables:**\n","   - **LabelEncoder:** Converts categorical labels into numerical values, allowing algorithms to process categorical data.\n","   - **OneHotEncoder:** Transforms categorical variables into a series of binary columns, effectively representing each category as a separate feature.\n","\n","4. **Binarization:**\n","   - **Binarizer:** Applies a threshold to numerical features, converting values above the threshold to 1 and those below to 0. This technique is useful for feature engineering when creating binary attributes from continuous data.\n","\n","5. **Imputation:**\n","   - **SimpleImputer:** Fills in missing values using a specified strategy, such as replacing missing entries with the mean, median, or most frequent value of feature."],"metadata":{"id":"6RzRuetgWHV9"}},{"cell_type":"markdown","source":["**Q9:What is a Test set?**\n","\n","In machine learning, a **test set** is a crucial component used to evaluate the performance of a trained model. After a model has been trained on a **training set**—the data it learns from—it is then assessed using the test set, which consists of data the model hasn't encountered before. This evaluation helps determine how well the model generalizes to new, unseen data.\n","\n","**Purpose of a Test Set:**\n","\n","- **Unbiased Evaluation:** The test set provides an unbiased assessment of the model's predictive capabilities, ensuring that the performance metrics reflect the model's ability to handle real-world data.\n","\n","- **Generalization Assessment:** By evaluating the model on data it hasn't seen during training, we can gauge its generalization ability—its effectiveness in making accurate predictions on new inputs.\n","\n","**Data Splitting:**\n","\n","Typically, the original dataset is divided into:\n","\n","- **Training Set:** Used to train the model, usually comprising the majority of the data.\n","\n","- **Test Set:** Set aside for final evaluation after training is complete.\n","\n","In some cases, a **validation set** is also used during training to fine-tune model parameters and prevent overfitting. The validation set assists in model selection and hyperparameter tuning without compromising the integrity of the test set.\n","\n","**Importance of the Test Set:**\n","\n","Utilizing a test set is vital to ensure that the model not only performs well on the data it was trained on but also maintains its accuracy when applied to new, unseen data. This practice helps in identifying issues like overfitting, where a model performs exceptionally on training data but poorly on external data."],"metadata":{"id":"HYS6uiJaXphM"}},{"cell_type":"markdown","source":["**Q10:How do we split data for model fitting (training and testing) in Python?\n","How do you approach a Machine Learning problem?**\n","\n","Splitting your dataset into training and testing subsets is a fundamental step in building and evaluating machine learning models. In Python, this is efficiently handled using the `train_test_split` function from the `scikit-learn` library.\n","\n","**Using `train_test_split` in Python:**\n","\n"," spliting of data:\n","\n","\n","```python\n","from sklearn.model_selection import train_test_split\n","\n","# Assuming X contains features and y contains the target variable\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","```\n","\n","\n","Example:\n","\n","- `X` and `y`: Represent your feature matrix and target vector, respectively.\n","- `test_size=0.2`: Allocates 20% of the data to the test set and 80% to the training set.\n","- `random_state=42`: Ensures reproducibility by setting a seed for the random number generator.\n","\n","This function randomly shuffles and splits the data into training and testing sets, which is crucial for unbiased model evaluation.\n","\n","**Approaching a Machine Learning Problem:**\n","\n","When tackling a machine learning problem, consider the following structured approach:\n","\n","1. **Define the Problem:**\n","   - Clearly articulate the problem you're aiming to solve.\n","   - Determine if machine learning is the appropriate solution.\n","\n","2. **Collect and Understand the Data:**\n","   - Gather relevant data from reliable sources.\n","   - Perform exploratory data analysis to comprehend the dataset's structure and characteristics.\n","\n","3. **Preprocess the Data:**\n","   - Handle missing values and outliers.\n","   - Encode categorical variables and scale numerical features as needed.\n","\n","4. **Split the Data:**\n","   - Divide the dataset into training and testing sets using `train_test_split` to evaluate model performance effectively.\n","\n","5. **Select and Train Models:**\n","   - Choose appropriate machine learning algorithms based on the problem type (e.g., regression, classification).\n","   - Train multiple models to compare performance.\n","\n","6. **Evaluate Models:**\n","   - Use metrics like accuracy, precision, recall, and F1-score for classification tasks, or RMSE for regression tasks, to assess model performance.\n","   - Perform cross-validation to ensure model robustness.\n","\n","7. **Tune Hyperparameters:**\n","   - Optimize model parameters using techniques like grid search or random search to enhance performance.\n","\n","8. **Test the Model:**\n","   - Evaluate the final model on the test set to assess its generalization to unseen data.\n","\n","9. **Deploy and Monitor:**\n","   - Implement the model into a production environment.\n","   - Continuously monitor its performance and update it as necessary to maintain accuracy over time.\n"],"metadata":{"id":"nI2jZClvX8fP"}},{"cell_type":"markdown","source":["**Q11:Why do we have to perform EDA before fitting a model to the data?**\n","\n","Engaging in **Exploratory Data Analysis (EDA)** before fitting a model is a fundamental step in the data science process. EDA involves examining and visualizing data to uncover patterns, detect anomalies, and test assumptions. This preliminary analysis is crucial for several reasons:\n","\n","1. **Understanding Data Structure:**\n","   EDA provides insights into the dataset's composition, including the types of variables, their distributions, and relationships. This understanding aids in selecting appropriate modeling techniques.\n","\n","2. **Identifying Anomalies and Outliers:**\n","   Through visualization methods like box plots and scatter plots, EDA helps detect outliers and anomalies that could skew model performance if left unaddressed.\n","\n","3. **Handling Missing Data:**\n","   EDA reveals patterns of missing data, enabling informed decisions on how to handle them—whether through imputation or exclusion—to maintain the integrity of the analysis.\n","\n","4. **Assessing Assumptions:**\n","   Many statistical models have underlying assumptions (e.g., normality, linearity). EDA allows for testing these assumptions, ensuring that the chosen models are appropriate for the data.\n","\n","5. **Informing Feature Selection and Engineering:**\n","   By exploring variable relationships and importance, EDA guides the selection of relevant features and the creation of new ones, enhancing model effectiveness.\n"],"metadata":{"id":"wYzPa_NXYoCq"}},{"cell_type":"markdown","source":["**Q12:What is correlation?**\n","\n","Correlation measures the statistical relationship between two variables, indicating how one variable changes in relation to another. It quantifies both the strength and direction of this relationship. The correlation coefficient, often denoted as r, ranges from -1 to 1:​\n","\n","**-->** Perfect positive correlation; as one variable increases, the other also increases proportionally.​\n","\n","**-->** Perfect negative correlation; as one variable increases, the other decreases proportionally.​\n","\n","**-->** No linear correlation; the variables do not have a linear relationship."],"metadata":{"id":"iyreX3swY6Md"}},{"cell_type":"markdown","source":["**Q13:What does negative correlation mean?**\n","\n","A negative correlation indicates that as one variable increases, the other decreases, and vice versa. For example, consider the relationship between the speed of a car and the time taken to reach a destination: as speed increases, travel time decreases. This inverse relationship is characterized by a negative correlation coefficient."],"metadata":{"id":"2tG_COyBZKPJ"}},{"cell_type":"code","source":["#Q14:How can you find correlation between variables in Python?\n","import pandas as pd\n","\n","# Create a DataFrame\n","data = {'Variable1': [10, 20, 30, 40, 50],\n","        'Variable2': [15, 24, 33, 48, 55]}\n","df = pd.DataFrame(data)\n","\n","# Calculate the correlation matrix\n","correlation_matrix = df.corr()\n","\n","print(correlation_matrix)\n","\n","#This will output a correlation matrix showing the pairwise correlation coefficients between the variables.\n","#The corr() function by default computes the Pearson correlation coefficient.\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T5m9L1ipZ4C7","executionInfo":{"status":"ok","timestamp":1743310548018,"user_tz":-330,"elapsed":1846,"user":{"displayName":"Aanisha GN","userId":"17686288113042909782"}},"outputId":"4c263c18-a00b-46f1-d2e6-de87dcc166e3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["           Variable1  Variable2\n","Variable1   1.000000   0.994317\n","Variable2   0.994317   1.000000\n"]}]},{"cell_type":"markdown","source":["**Q15:What is causation? Explain difference between correlation and causation with an example**\n","\n","Causation implies a cause-and-effect relationship where one event is the result of the occurrence of the other. In other words, one variable directly affects the other.​\n","\n","**Difference Between Correlation and Causation:**\n","\n","**Correlation:** Indicates that two variables move together but does not establish a cause-and-effect relationship.​\n","\n","**Causation:** Establishes that one variable directly affects the other.​\n","\n","**Example:**\n","\n","Consider a study that finds a correlation between ice cream sales and drowning incidents; both tend to increase during summer months. However, this does not mean ice cream consumption causes drowning. The underlying factor is the hot weather, which leads to both higher ice cream sales and more swimming activities, thereby increasing the risk of drowning. This illustrates that correlation does not imply causation."],"metadata":{"id":"obxr96jMZRlS"}},{"cell_type":"markdown","source":["**Q16:What is an Optimizer? What are different types of optimizers? Explain each with an example.**\n","\n","In machine learning, an optimizer is an algorithm that adjusts the parameters of a model to minimize the loss function, thereby improving the model's performance. Optimizers play a crucial role in training neural networks by updating weights and biases to reduce errors.​\n","\n","**Types of Optimizers:**\n","\n","**a. Gradient Descent (GD):**\n","\n","This is the foundational optimization algorithm that updates parameters by moving in the direction of the negative gradient of the loss function. The update rule is:​\n","\n","θ = θ - η * ∇J(θ)​\n","\n","where θ represents the parameters, η is the learning rate, and ∇J(θ) is the gradient of the loss function.​\n","\n","**Example:**\n","\n","In linear regression, gradient descent is used to find the line of best fit by minimizing the mean squared error between the predicted and actual values.​\n","\n","**b. Stochastic Gradient Descent (SGD):**\n","\n","SGD updates the model parameters using only one or a few training examples at each iteration. This approach introduces noise into the optimization process but can lead to faster convergence.​\n","\n","**Example:**\n","\n","In large-scale machine learning tasks, such as training deep neural networks, SGD is preferred due to its computational efficiency.​\n","\n","**c. Mini-Batch Gradient Descent:**\n","\n","This variant splits the training data into small batches and performs an update for each batch. It balances the efficiency of SGD and the stability of batch gradient descent.​\n","\n","**Example:**\n","\n","Training convolutional neural networks often employs mini-batch gradient descent to leverage parallel processing capabilities of modern hardware.​\n","\n","**d. Momentum:**\n","\n","Momentum accelerates gradient descent by considering the past gradients to smooth out the updates, leading to faster convergence and reduced oscillations.​\n","\n","**Example:**\n","\n","In training deep networks, momentum helps in navigating ravines and avoiding local minima.​\n","\n","**e. Adaptive Gradient Algorithm (AdaGrad):**\n","\n","AdaGrad adapts the learning rate for each parameter based on the historical gradients, allowing for larger updates for infrequent parameters and smaller updates for frequent ones.​\n","\n","**Example:**\n","\n","AdaGrad is effective in natural language processing tasks where some features occur infrequently.​\n","\n","**f. RMSprop:**\n","\n","RMSprop addresses AdaGrad's diminishing learning rates by maintaining a moving average of squared gradients and normalizing the gradient by this average.​\n","\n","**Example:**\n","\n","RMSprop is widely used in training recurrent neural networks.​\n","\n","**g. Adam (Adaptive Moment Estimation):**\n","\n","Adam combines the benefits of momentum and RMSprop by computing adaptive learning rates for each parameter. It maintains running averages of both gradients and their squared values.​\n","\n","**Example:**\n","\n","Adam is popular in various deep learning applications due to its robustness and efficiency.​\n"],"metadata":{"id":"y8Hjn3H8ZVri"}},{"cell_type":"markdown","source":["**Q17:What is sklearn.linear_model ?**\n","\n","The sklearn.linear_model module in scikit-learn provides a collection of linear models for regression and classification tasks. These models assume a linear relationship between input features and the target variable. A prominent example is the LinearRegression class, which implements Ordinary Least Squares (OLS) regression. This method aims to find the best-fitting line by minimizing the residual sum of squares between observed and predicted values."],"metadata":{"id":"23JWw2gja9LB"}},{"cell_type":"code","source":["#Q18:What does model.fit() do? What arguments must be given?\n","\n","'''The fit() method trains a machine learning model on provided data. It adjusts the model's parameters to learn patterns within the dataset.\n","or instance, in linear regression, fit() calculates the optimal coefficients for the input features. ​\n","\n","Arguments for fit():\n","\n","X: A 2D array-like structure containing the training data (features). Each row represents an instance, and each column represents a feature.​\n","\n","y: A 1D array-like structure containing the target values corresponding to each instance in X.'''\n","\n","from sklearn.linear_model import LinearRegression\n","\n","# Sample training data\n","X_train = [[1], [2], [3], [4], [5]]\n","y_train = [2, 4, 6, 8, 10]\n","\n","# Initialize and train the model\n","model = LinearRegression()\n","model.fit(X_train, y_train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":80},"id":"ZlG2GodYzYQ_","executionInfo":{"status":"ok","timestamp":1743317268070,"user_tz":-330,"elapsed":6170,"user":{"displayName":"Aanisha GN","userId":"17686288113042909782"}},"outputId":"52123305-b46f-4754-94cf-defa781ae89f"},"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LinearRegression()"],"text/html":["<style>#sk-container-id-1 {\n","  /* Definition of color scheme common for light and dark mode */\n","  --sklearn-color-text: #000;\n","  --sklearn-color-text-muted: #666;\n","  --sklearn-color-line: gray;\n","  /* Definition of color scheme for unfitted estimators */\n","  --sklearn-color-unfitted-level-0: #fff5e6;\n","  --sklearn-color-unfitted-level-1: #f6e4d2;\n","  --sklearn-color-unfitted-level-2: #ffe0b3;\n","  --sklearn-color-unfitted-level-3: chocolate;\n","  /* Definition of color scheme for fitted estimators */\n","  --sklearn-color-fitted-level-0: #f0f8ff;\n","  --sklearn-color-fitted-level-1: #d4ebff;\n","  --sklearn-color-fitted-level-2: #b3dbfd;\n","  --sklearn-color-fitted-level-3: cornflowerblue;\n","\n","  /* Specific color for light theme */\n","  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n","  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n","  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n","  --sklearn-color-icon: #696969;\n","\n","  @media (prefers-color-scheme: dark) {\n","    /* Redefinition of color scheme for dark theme */\n","    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n","    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n","    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n","    --sklearn-color-icon: #878787;\n","  }\n","}\n","\n","#sk-container-id-1 {\n","  color: var(--sklearn-color-text);\n","}\n","\n","#sk-container-id-1 pre {\n","  padding: 0;\n","}\n","\n","#sk-container-id-1 input.sk-hidden--visually {\n","  border: 0;\n","  clip: rect(1px 1px 1px 1px);\n","  clip: rect(1px, 1px, 1px, 1px);\n","  height: 1px;\n","  margin: -1px;\n","  overflow: hidden;\n","  padding: 0;\n","  position: absolute;\n","  width: 1px;\n","}\n","\n","#sk-container-id-1 div.sk-dashed-wrapped {\n","  border: 1px dashed var(--sklearn-color-line);\n","  margin: 0 0.4em 0.5em 0.4em;\n","  box-sizing: border-box;\n","  padding-bottom: 0.4em;\n","  background-color: var(--sklearn-color-background);\n","}\n","\n","#sk-container-id-1 div.sk-container {\n","  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n","     but bootstrap.min.css set `[hidden] { display: none !important; }`\n","     so we also need the `!important` here to be able to override the\n","     default hidden behavior on the sphinx rendered scikit-learn.org.\n","     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n","  display: inline-block !important;\n","  position: relative;\n","}\n","\n","#sk-container-id-1 div.sk-text-repr-fallback {\n","  display: none;\n","}\n","\n","div.sk-parallel-item,\n","div.sk-serial,\n","div.sk-item {\n","  /* draw centered vertical line to link estimators */\n","  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n","  background-size: 2px 100%;\n","  background-repeat: no-repeat;\n","  background-position: center center;\n","}\n","\n","/* Parallel-specific style estimator block */\n","\n","#sk-container-id-1 div.sk-parallel-item::after {\n","  content: \"\";\n","  width: 100%;\n","  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n","  flex-grow: 1;\n","}\n","\n","#sk-container-id-1 div.sk-parallel {\n","  display: flex;\n","  align-items: stretch;\n","  justify-content: center;\n","  background-color: var(--sklearn-color-background);\n","  position: relative;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item {\n","  display: flex;\n","  flex-direction: column;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item:first-child::after {\n","  align-self: flex-end;\n","  width: 50%;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item:last-child::after {\n","  align-self: flex-start;\n","  width: 50%;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item:only-child::after {\n","  width: 0;\n","}\n","\n","/* Serial-specific style estimator block */\n","\n","#sk-container-id-1 div.sk-serial {\n","  display: flex;\n","  flex-direction: column;\n","  align-items: center;\n","  background-color: var(--sklearn-color-background);\n","  padding-right: 1em;\n","  padding-left: 1em;\n","}\n","\n","\n","/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n","clickable and can be expanded/collapsed.\n","- Pipeline and ColumnTransformer use this feature and define the default style\n","- Estimators will overwrite some part of the style using the `sk-estimator` class\n","*/\n","\n","/* Pipeline and ColumnTransformer style (default) */\n","\n","#sk-container-id-1 div.sk-toggleable {\n","  /* Default theme specific background. It is overwritten whether we have a\n","  specific estimator or a Pipeline/ColumnTransformer */\n","  background-color: var(--sklearn-color-background);\n","}\n","\n","/* Toggleable label */\n","#sk-container-id-1 label.sk-toggleable__label {\n","  cursor: pointer;\n","  display: flex;\n","  width: 100%;\n","  margin-bottom: 0;\n","  padding: 0.5em;\n","  box-sizing: border-box;\n","  text-align: center;\n","  align-items: start;\n","  justify-content: space-between;\n","  gap: 0.5em;\n","}\n","\n","#sk-container-id-1 label.sk-toggleable__label .caption {\n","  font-size: 0.6rem;\n","  font-weight: lighter;\n","  color: var(--sklearn-color-text-muted);\n","}\n","\n","#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n","  /* Arrow on the left of the label */\n","  content: \"▸\";\n","  float: left;\n","  margin-right: 0.25em;\n","  color: var(--sklearn-color-icon);\n","}\n","\n","#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n","  color: var(--sklearn-color-text);\n","}\n","\n","/* Toggleable content - dropdown */\n","\n","#sk-container-id-1 div.sk-toggleable__content {\n","  max-height: 0;\n","  max-width: 0;\n","  overflow: hidden;\n","  text-align: left;\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-toggleable__content.fitted {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-toggleable__content pre {\n","  margin: 0.2em;\n","  border-radius: 0.25em;\n","  color: var(--sklearn-color-text);\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n","  /* Expand drop-down */\n","  max-height: 200px;\n","  max-width: 100%;\n","  overflow: auto;\n","}\n","\n","#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n","  content: \"▾\";\n","}\n","\n","/* Pipeline/ColumnTransformer-specific style */\n","\n","#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Estimator-specific style */\n","\n","/* Colorize estimator box */\n","#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n","#sk-container-id-1 div.sk-label label {\n","  /* The background is the default theme color */\n","  color: var(--sklearn-color-text-on-default-background);\n","}\n","\n","/* On hover, darken the color of the background */\n","#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","/* Label box, darken color on hover, fitted */\n","#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Estimator label */\n","\n","#sk-container-id-1 div.sk-label label {\n","  font-family: monospace;\n","  font-weight: bold;\n","  display: inline-block;\n","  line-height: 1.2em;\n","}\n","\n","#sk-container-id-1 div.sk-label-container {\n","  text-align: center;\n","}\n","\n","/* Estimator-specific */\n","#sk-container-id-1 div.sk-estimator {\n","  font-family: monospace;\n","  border: 1px dotted var(--sklearn-color-border-box);\n","  border-radius: 0.25em;\n","  box-sizing: border-box;\n","  margin-bottom: 0.5em;\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-estimator.fitted {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","/* on hover */\n","#sk-container-id-1 div.sk-estimator:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-estimator.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Specification for estimator info (e.g. \"i\" and \"?\") */\n","\n","/* Common style for \"i\" and \"?\" */\n","\n",".sk-estimator-doc-link,\n","a:link.sk-estimator-doc-link,\n","a:visited.sk-estimator-doc-link {\n","  float: right;\n","  font-size: smaller;\n","  line-height: 1em;\n","  font-family: monospace;\n","  background-color: var(--sklearn-color-background);\n","  border-radius: 1em;\n","  height: 1em;\n","  width: 1em;\n","  text-decoration: none !important;\n","  margin-left: 0.5em;\n","  text-align: center;\n","  /* unfitted */\n","  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-unfitted-level-1);\n","}\n","\n",".sk-estimator-doc-link.fitted,\n","a:link.sk-estimator-doc-link.fitted,\n","a:visited.sk-estimator-doc-link.fitted {\n","  /* fitted */\n","  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-fitted-level-1);\n","}\n","\n","/* On hover */\n","div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",".sk-estimator-doc-link:hover,\n","div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",".sk-estimator-doc-link:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",".sk-estimator-doc-link.fitted:hover,\n","div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",".sk-estimator-doc-link.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","/* Span, style for the box shown on hovering the info icon */\n",".sk-estimator-doc-link span {\n","  display: none;\n","  z-index: 9999;\n","  position: relative;\n","  font-weight: normal;\n","  right: .2ex;\n","  padding: .5ex;\n","  margin: .5ex;\n","  width: min-content;\n","  min-width: 20ex;\n","  max-width: 50ex;\n","  color: var(--sklearn-color-text);\n","  box-shadow: 2pt 2pt 4pt #999;\n","  /* unfitted */\n","  background: var(--sklearn-color-unfitted-level-0);\n","  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n","}\n","\n",".sk-estimator-doc-link.fitted span {\n","  /* fitted */\n","  background: var(--sklearn-color-fitted-level-0);\n","  border: var(--sklearn-color-fitted-level-3);\n","}\n","\n",".sk-estimator-doc-link:hover span {\n","  display: block;\n","}\n","\n","/* \"?\"-specific style due to the `<a>` HTML tag */\n","\n","#sk-container-id-1 a.estimator_doc_link {\n","  float: right;\n","  font-size: 1rem;\n","  line-height: 1em;\n","  font-family: monospace;\n","  background-color: var(--sklearn-color-background);\n","  border-radius: 1rem;\n","  height: 1rem;\n","  width: 1rem;\n","  text-decoration: none;\n","  /* unfitted */\n","  color: var(--sklearn-color-unfitted-level-1);\n","  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n","}\n","\n","#sk-container-id-1 a.estimator_doc_link.fitted {\n","  /* fitted */\n","  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-fitted-level-1);\n","}\n","\n","/* On hover */\n","#sk-container-id-1 a.estimator_doc_link:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-3);\n","}\n","</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LinearRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LinearRegression.html\">?<span>Documentation for LinearRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LinearRegression()</pre></div> </div></div></div></div>"]},"metadata":{},"execution_count":1}]},{"cell_type":"code","source":["#Q19:What does model.predict() do? What arguments must be given?\n","'''The predict() method generates predictions using the trained model on new, unseen data.\n","It applies the learned patterns from the training phase to estimate outcomes for input features. ​\n","\n","Arguments for predict():\n","\n","X: A 2D array-like structure containing the input features for which predictions are to be made.\n","The structure should match the format used during training.'''\n","# Sample test data\n","X_test = [[6], [7], [8]]\n","\n","# Generate predictions\n","predictions = model.predict(X_test)\n","print(predictions)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eGG6VgCyzrK7","executionInfo":{"status":"ok","timestamp":1743317326354,"user_tz":-330,"elapsed":10,"user":{"displayName":"Aanisha GN","userId":"17686288113042909782"}},"outputId":"8a431416-9768-4783-8cac-c684c3d5ef67"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["[12. 14. 16.]\n"]}]},{"cell_type":"markdown","source":["**Q20:What are continuous and categorical variables?**\n","\n","**Continuous Variables:** These are numerical variables that can take an infinite number of values within a range. Examples include height, weight, and temperature. Continuous data allows for fractional values and is suitable for mathematical operations.​\n","\n","**Categorical Variables:**These variables represent discrete categories or groups without inherent numerical meaning. Examples include colors, gender, and types of cuisine. Categorical data can be nominal (unordered categories) or ordinal (ordered categories)."],"metadata":{"id":"Ecq9Y-mUycHZ"}},{"cell_type":"markdown","source":["**Q21:What is feature scaling? How does it help in Machine Learning?**\n","\n","Feature scaling is a preprocessing technique that standardizes the range of independent variables or features in your data. It ensures that all features contribute equally to the model, preventing features with larger ranges from dominating those with smaller ranges.​\n","\n","**Benefits of Feature Scaling:**\n","\n","**Improved Model Performance:** Scaling can enhance the performance of algorithms sensitive to feature magnitudes, such as gradient descent-based models.​\n","\n","**Faster Convergence:** Algorithms converge more quickly when features are on similar scales, reducing computation time.​\n","\n","**Increased Accuracy:**Equalizing feature influence can lead to more accurate and reliable models.​\n","\n","Feature scaling is particularly important for distance-based algorithms like k-Nearest Neighbors and support vector machines."],"metadata":{"id":"EOLqLB9JaSjp"}},{"cell_type":"code","source":["#Q:22 How do we perform scaling in Python?\n","from sklearn.preprocessing import StandardScaler\n","import pandas as pd\n","\n","# Sample data\n","data = {'Feature1': [10, 20, 30, 40, 50],\n","        'Feature2': [15, 25, 35, 45, 55]}\n","df = pd.DataFrame(data)\n","\n","# Initialize the scaler\n","scaler = StandardScaler()\n","\n","# Fit and transform the data\n","scaled_data = scaler.fit_transform(df)\n","\n","# Convert the scaled data back into a DataFrame\n","scaled_df = pd.DataFrame(scaled_data, columns=df.columns)\n","print(scaled_df)\n","\n","'''Standardization (Z-score Normalization):\n","\n","This method transforms the data so that it has a mean (μ) of 0 and a standard deviation (σ) of 1, effectively centering the\n","data around zero with a unit standard deviation.​"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qaYlqaYQ0SHF","executionInfo":{"status":"ok","timestamp":1743317789661,"user_tz":-330,"elapsed":50,"user":{"displayName":"Aanisha GN","userId":"17686288113042909782"}},"outputId":"b25221c6-f78f-4499-e891-db28f01b0d9e"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["   Feature1  Feature2\n","0 -1.414214 -1.414214\n","1 -0.707107 -0.707107\n","2  0.000000  0.000000\n","3  0.707107  0.707107\n","4  1.414214  1.414214\n"]}]},{"cell_type":"markdown","source":["**Q23:What is sklearn.preprocessing?**\n","\n","sklearn.preprocessing is a module within the scikit-learn library that offers various utilities and transformer classes for preprocessing data. These tools are essential for transforming raw data into a format that is more suitable for modeling, thereby enhancing the performance and accuracy of machine learning algorithms.\n","\n","**Key functionalities of sklearn.preprocessing include:**\n","\n","**Scaling and Centering:** Standardizing features by removing the mean and scaling to unit variance using classes like StandardScaler.​\n","Scikit-learn\n","\n","**Normalization:** Adjusting the data to a standard scale without distorting differences in the ranges of values, often using Normalizer.​\n","\n","**Binarization:** Converting numerical values into binary (0 or 1) based on a threshold using Binarizer.​\n","\n","**Encoding Categorical Features:** Transforming categorical variables into numerical formats using techniques like One-Hot Encoding (OneHotEncoder) and Label Encoding (LabelEncoder).​\n","\n","\n","**Imputation:** Handling missing values by replacing them with statistical measures like mean, median, or a constant value using SimpleImputer.​\n","\n","These preprocessing steps are crucial as many machine learning algorithms require numerical input and may perform poorly if the data is not properly scaled or encoded."],"metadata":{"id":"ei7ltHldyn_8"}},{"cell_type":"code","source":["#Q24:How do we split data for model fitting (training and testing) in Python?\n","\n","#Splitting data into training and testing sets is a fundamental step in evaluating the performance of a machine learning model.\n","#The training set is used to train the model, while the testing set assesses its performance on unseen data.​\n","\n","#Using train_test_split from scikit-learn:\n","\n","#The train_test_split function from sklearn.model_selection is commonly used to split data into training and testing sets.\n","\n","from sklearn.model_selection import train_test_split\n","\n","import pandas as pd\n","\n","# Sample data\n","data = {\n","    'Feature1': [10, 20, 30, 40, 50],\n","    'Feature2': [15, 25, 35, 45, 55],\n","    'Target': [0, 1, 0, 1, 0]\n","}\n","df = pd.DataFrame(data)\n","\n","# Features and target\n","X = df[['Feature1', 'Feature2']]\n","y = df['Target']\n","\n","# Splitting data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","print(\"Training Features:\\n\", X_train)\n","print(\"Testing Features:\\n\", X_test)\n","print(\"Training Target:\\n\", y_train)\n","print(\"Testing Target:\\n\", y_test)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-aLUKLpK2lyl","executionInfo":{"status":"ok","timestamp":1743318061043,"user_tz":-330,"elapsed":22,"user":{"displayName":"Aanisha GN","userId":"17686288113042909782"}},"outputId":"409734a2-a958-423f-d4b5-306b9f019aaa"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Features:\n","    Feature1  Feature2\n","4        50        55\n","2        30        35\n","0        10        15\n","3        40        45\n","Testing Features:\n","    Feature1  Feature2\n","1        20        25\n","Training Target:\n"," 4    0\n","2    0\n","0    0\n","3    1\n","Name: Target, dtype: int64\n","Testing Target:\n"," 1    1\n","Name: Target, dtype: int64\n"]}]},{"cell_type":"markdown","source":["**Q25:Explain data encoding?**\n","\n","\n","In machine learning, **data encoding** is the process of converting categorical (non-numeric) data into numerical formats so that algorithms can process and learn from it. Since most machine learning models work with numerical data, encoding categorical variables is an essential preprocessing step.  \n","\n","---\n","### **Why is Data Encoding Needed?**  \n","Many real-world datasets contain categorical data, such as:  \n","- **Nominal categories** (e.g., colors: \"Red\", \"Blue\", \"Green\")  \n","- **Ordinal categories** (e.g., education levels: \"High School\", \"Bachelor's\", \"Master's\", \"PhD\")  \n","\n","Since machine learning models interpret numbers mathematically, feeding them raw categorical data can cause errors or incorrect assumptions. Data encoding helps convert these categories into a format that models can understand.\n","\n","---\n","### **Types of Data Encoding Techniques**\n","\n","#### **1. Label Encoding**  \n","- Assigns a unique integer to each category.  \n","- Suitable for **ordinal** data (where categories have a meaningful order).  \n","\n","**Example:**  \n","```python\n","from sklearn.preprocessing import LabelEncoder\n","\n","# Sample categorical data\n","data = ['low', 'medium', 'high', 'medium', 'low']\n","\n","# Initialize the encoder\n","encoder = LabelEncoder()\n","\n","# Fit and transform the data\n","encoded_data = encoder.fit_transform(data)\n","print(encoded_data)\n","```\n","**Output:**  \n","```\n","[1 2 0 2 1]\n","```\n","Here, ‘low’ = 1, ‘medium’ = 2, and ‘high’ = 0.\n","\n"," **Problem:** If categories have no order (e.g., \"Apple\", \"Banana\", \"Cherry\"), assigning numbers can create a false sense of ranking.\n","\n","---\n","#### **2. One-Hot Encoding (OHE)**  \n","- Creates **binary columns** for each category, where 1 indicates the presence and 0 indicates absence.  \n","- Suitable for **nominal** data (where categories have no meaningful order).  \n","\n","**Example:**  \n","```python\n","from sklearn.preprocessing import OneHotEncoder\n","import pandas as pd\n","\n","# Sample categorical data\n","data = pd.DataFrame({'Color': ['Red', 'Blue', 'Green', 'Blue', 'Red']})\n","\n","# Initialize encoder\n","encoder = OneHotEncoder(sparse=False)\n","\n","# Fit and transform\n","encoded_data = encoder.fit_transform(data[['Color']])\n","print(encoded_data)\n","```\n","**Output:**  \n","```\n","[[0. 0. 1.]\n"," [1. 0. 0.]\n"," [0. 1. 0.]\n"," [1. 0. 0.]\n"," [0. 0. 1.]]\n","```\n","Each row now has a separate binary representation for the color.\n","\n"," **Problem:** If there are too many categories (e.g., thousands of unique values), one-hot encoding creates a very large number of columns, increasing memory usage.\n","\n","---\n","#### **3. Ordinal Encoding**  \n","- Similar to label encoding but maintains a **specific order** among categories.  \n","- Used when categories have an inherent ranking (e.g., \"Beginner\" < \"Intermediate\" < \"Advanced\").  \n","\n","**Example:**  \n","```python\n","from sklearn.preprocessing import OrdinalEncoder\n","\n","data = pd.DataFrame({'Size': ['Small', 'Medium', 'Large', 'Small', 'Large']})\n","\n","# Define the order of categories\n","encoder = OrdinalEncoder(categories=[['Small', 'Medium', 'Large']])\n","\n","# Fit and transform\n","encoded_data = encoder.fit_transform(data[['Size']])\n","print(encoded_data)\n","```\n","**Output:**  \n","```\n","[[0.]\n"," [1.]\n"," [2.]\n"," [0.]\n"," [2.]]\n","```\n","Here, ‘Small’ = 0, ‘Medium’ = 1, and ‘Large’ = 2.\n","\n","---\n","#### **4. Target Encoding (Mean Encoding)**  \n","- Replaces each category with the **mean of the target variable** (works best for classification problems).  \n","- Used when categorical data has a **strong correlation** with the target variable.  \n","\n","**Example:**  \n","```python\n","import pandas as pd\n","\n","# Sample data\n","df = pd.DataFrame({\n","    'Category': ['A', 'B', 'A', 'B', 'A', 'C', 'C', 'B'],\n","    'Target': [1, 0, 1, 0, 1, 0, 1, 0]\n","})\n","\n","# Compute mean target value per category\n","target_mean = df.groupby('Category')['Target'].mean()\n","\n","# Replace categories with target mean\n","df['Encoded'] = df['Category'].map(target_mean)\n","print(df)\n","```\n","**Output:**  \n","```\n","  Category  Target  Encoded\n","0       A       1      1.00\n","1       B       0      0.00\n","2       A       1      1.00\n","3       B       0      0.00\n","4       A       1      1.00\n","5       C       0      0.50\n","6       C       1      0.50\n","7       B       0      0.00\n","```\n","⚠️ **Problem:** Can lead to **data leakage** if computed on the full dataset. Use cross-validation to avoid this issue.\n","\n","---\n","#### **5. Frequency Encoding (Count Encoding)**  \n","- Replaces categories with their **frequency (count) in the dataset**.  \n","\n","**Example:**  \n","```python\n","df['Frequency_Encoded'] = df['Category'].map(df['Category'].value_counts())\n","print(df)\n","```\n","**Output:**  \n","```\n","  Category  Target  Frequency_Encoded\n","0       A       1                  3\n","1       B       0                  3\n","2       A       1                  3\n","3       B       0                  3\n","4       A       1                  3\n","5       C       0                  2\n","6       C       1                  2\n","7       B       0                  3\n","```\n","**Problem:** If category frequencies are similar, the model may struggle to differentiate them.\n","\n","---\n","### **Choosing the Right Encoding Method**\n","| Encoding Method | Suitable For | Potential Issues |\n","|----------------|-------------|------------------|\n","| **Label Encoding** | Ordinal categories (e.g., \"low\", \"medium\", \"high\") | Implies a false ranking if used on nominal data |\n","| **One-Hot Encoding** | Nominal categories (e.g., \"red\", \"blue\", \"green\") | Increases memory usage if too many unique values |\n","| **Ordinal Encoding** | Ordered categories (e.g., \"beginner\", \"expert\") | Works poorly if categories don’t have a real ranking |\n","| **Target Encoding** | Categorical features strongly related to target | Risk of **data leakage** |\n","| **Frequency Encoding** | High-cardinality categorical features | Doesn’t retain order information |\n","\n","---\n","### **Conclusion**  \n","- **For small categorical features**, **one-hot encoding** is usually the best option.  \n","- **For large categorical features**, **target encoding** or **frequency encoding** is often preferred.  \n","- **For ordinal data**, **ordinal encoding** is the best choice.  \n","\n","**Choosing the right encoding method** ensures better model performance while avoiding issues like overfitting, memory overload, and false numerical relationships."],"metadata":{"id":"VdTDV8V6ywX-"}}]}